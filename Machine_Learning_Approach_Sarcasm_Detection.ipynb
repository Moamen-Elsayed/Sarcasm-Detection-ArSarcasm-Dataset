{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HZiaFTkapmC"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1241,
     "status": "ok",
     "timestamp": 1622967124840,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "A977waCudCMB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "import gensim\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import emojis\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1622967234469,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "qtF0tCZgxxhu"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"data/ArSarcasm_train.csv\")\n",
    "df_test = pd.read_csv(r\"data/ArSarcasm_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31487,
     "status": "ok",
     "timestamp": 1622967156319,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "qxpNLkUo1mns",
    "outputId": "a19ac9aa-33e5-488f-e72e-90f75dfaf70e"
   },
   "outputs": [],
   "source": [
    "stemmer =  FarasaStemmer(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1622967241306,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "leli8-t_pNst"
   },
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = df_train['tweet'].apply(helper.clean_tweet)\n",
    "df_test['cleaned_tweet'] = df_test['tweet'].apply(helper.clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14241,
     "status": "ok",
     "timestamp": 1622967257151,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "RSwU4YcbxzVG"
   },
   "outputs": [],
   "source": [
    "df_train['cleaned_tweet'] = df_train['cleaned_tweet'].apply(lambda x: stemmer.stem(x))\n",
    "df_test['cleaned_tweet'] = df_test['cleaned_tweet'].apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49Nv52Zm9w3_"
   },
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 7793,
     "status": "ok",
     "timestamp": 1622967366971,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "1z1EVSmr9SGQ",
    "outputId": "b5f3f081-8aa7-4bc6-fb18-54045017835e"
   },
   "outputs": [],
   "source": [
    "df_train[\"cleaned_tweet\"] = df_train[\"cleaned_tweet\"].apply(helper.remove_stopWords)\n",
    "df_test[\"cleaned_tweet\"] = df_test[\"cleaned_tweet\"].apply(helper.remove_stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEaWJd9T-HYQ"
   },
   "source": [
    "# Build our Vocab List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8441,
     "status": "ok",
     "timestamp": 1622967394284,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "3SFJgJDe9SC_"
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for tw in df_train['cleaned_tweet']:\n",
    "    for word in tw.split():\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1622967394292,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "cZT_hVuz9SAD",
    "outputId": "8e46218c-1c3e-4e18-fd43-6a9c328189cf"
   },
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PBhknYvZiT9"
   },
   "source": [
    "# Prepare Data to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1622967537101,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "vhVVreMx9R9H"
   },
   "outputs": [],
   "source": [
    "train_data = df_train[['sarcasm', 'cleaned_tweet']]\n",
    "test_data = df_test[['sarcasm', 'cleaned_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1622967539647,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "iCJUjyOJ9R6E",
    "outputId": "100e9bb8-5eeb-4115-efaa-8082531f2875"
   },
   "outputs": [],
   "source": [
    "train_data['sarcasm'] = train_data['sarcasm'].apply(lambda x: 1 if x==True else 0)\n",
    "test_data['sarcasm'] = test_data['sarcasm'].apply(lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1622968051531,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "UmMWD69-ayEy",
    "outputId": "e848f191-0651-4f6b-aa6c-0d1b14f14561"
   },
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(inplace=True)\n",
    "test_data.drop_duplicates(inplace=True)\n",
    "X_train = train_data['cleaned_tweet']\n",
    "X_test = test_data['cleaned_tweet']\n",
    "y_train = train_data['sarcasm'].tolist()\n",
    "y_test = test_data['sarcasm'].tolist()\n",
    "\n",
    "print('Size of train data: {}'.format(len(X_train)))\n",
    "print('Size of test data: {}'.format(len(X_test)))\n",
    "print('Size of train labels: {}'.format(len(y_train)))\n",
    "print('Size of test labels: {}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvcySRLac53l"
   },
   "source": [
    "## Embedding Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "executionInfo": {
     "elapsed": 1359,
     "status": "ok",
     "timestamp": 1622968208899,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "YStGklN99Rud",
    "outputId": "a2eac834-05e2-4cbc-c4f8-7b671150d393"
   },
   "outputs": [],
   "source": [
    "X_train_bow_emb, count_vectorizer = helper.bow_emb(X_train)\n",
    "X_test_bow_emb = count_vectorizer.transform(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "helper.plot_LSA(X_train_bow_emb, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_emb, tfidf_vectorizer = helper.tfidf(X_train)\n",
    "X_test_tfidf_emb = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "helper.plot_LSA(X_train_tfidf_emb, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"sg_100.bin\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, unicode_errors='ignore')   \n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "X_train_tokens = train_data['cleaned_tweet'].apply(tokenizer.tokenize)\n",
    "X_test_tokens = test_data['cleaned_tweet'].apply(tokenizer.tokenize)\n",
    "\n",
    "train_embeddings = helper.get_word2vec_embeddings(model, X_train_tokens)\n",
    "test_embeddings = helper.get_word2vec_embeddings(model, X_test_tokens)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))          \n",
    "helper.plot_LSA(train_embeddings, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1622971248805,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "Bwd83dWK9Rrj",
    "outputId": "5bb60741-271a-4b25-8957-148dbea1d5cd"
   },
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(solver='newton-cg', class_weight='balanced')\n",
    "lr_bow.fit(X_train_bow_emb, y_train)\n",
    "pred_train = lr_bow.predict(X_train_bow_emb)\n",
    "\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_bow.predict(X_test_bow_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqD884_voQ6d"
   },
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "executionInfo": {
     "elapsed": 146736,
     "status": "ok",
     "timestamp": 1622971760953,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "yJ9EUdUJ9Rfe",
    "outputId": "f1f9f2eb-f85d-47ad-fad8-7e0d63e320cc"
   },
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr_bow, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train_bow_emb, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bow = LogisticRegression(solver='newton-cg', class_weight = gridsearch.best_params_['class_weight'])\n",
    "lr_bow.fit(X_train_bow_emb, y_train)\n",
    "pred_train = lr_bow.predict(X_train_bow_emb)\n",
    "\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_bow.predict(X_test_bow_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = helper.get_most_important_features(count_vectorizer, lr_bow, 10)\n",
    "\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "helper.plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = LogisticRegression(solver='newton-cg', class_weight='balanced')\n",
    "lr_tfidf.fit(X_train_tfidf_emb, y_train)\n",
    "pred_train = lr_tfidf.predict(X_train_tfidf_emb)\n",
    "\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_tfidf.predict(X_test_tfidf_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr_tfidf, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train_tfidf_emb, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = LogisticRegression(solver='newton-cg', class_weight = gridsearch.best_params_['class_weight'])\n",
    "lr_tfidf.fit(X_train_tfidf_emb, y_train)\n",
    "\n",
    "pred_train = lr_tfidf.predict(X_train_tfidf_emb)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_tfidf.predict(X_test_tfidf_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = helper.get_most_important_features(tfidf_vectorizer, lr_tfidf, 10)\n",
    "\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "helper.plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with W2V Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w2v = LogisticRegression(solver='newton-cg', class_weight='balanced')\n",
    "lr_w2v.fit(train_embeddings, y_train)\n",
    "\n",
    "pred_train = lr_w2v.predict(train_embeddings)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_w2v.predict(test_embeddings)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w2v = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr_w2v, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(train_embeddings, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w2v = LogisticRegression(solver='newton-cg', class_weight=gridsearch.best_params_['class_weight'])\n",
    "lr_w2v.fit(train_embeddings, y_train)\n",
    "\n",
    "pred_train = lr_w2v.predict(train_embeddings)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = lr_w2v.predict(test_embeddings)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12367,
     "status": "ok",
     "timestamp": 1622972245891,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "352yRUCj9RWn",
    "outputId": "99abe844-d9a5-4335-c656-93e5d91ced04"
   },
   "outputs": [],
   "source": [
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(X_train_bow_emb, y_train)\n",
    "\n",
    "pred_train = svm.predict(X_train_bow_emb)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "pred_test = svm.predict(X_test_bow_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726
    },
    "executionInfo": {
     "elapsed": 164046,
     "status": "ok",
     "timestamp": 1622976974598,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "emssh7OXck_4",
    "outputId": "b6479c58-ac68-4b5b-9531-dd3d79f256ac"
   },
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= svm, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(n_splits=5), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train_bow_emb, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "executionInfo": {
     "elapsed": 13159,
     "status": "ok",
     "timestamp": 1622978819076,
     "user": {
      "displayName": "Moamen Elsayed",
      "photoUrl": "",
      "userId": "04018272351791962551"
     },
     "user_tz": 420
    },
    "id": "gfotRb4Qck6I",
    "outputId": "895bbce6-479f-4767-b9b1-980a51b2a90b"
   },
   "outputs": [],
   "source": [
    "svm = SVC(class_weight=gridsearch.best_params_['class_weight'])\n",
    "svm.fit(X_train_bow_emb, y_train)\n",
    "\n",
    "pred_train = svm.predict(X_train_bow_emb)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = svm.predict(X_test_bow_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(X_train_tfidf_emb, y_train)\n",
    "\n",
    "pred_train = svm.predict(X_train_tfidf_emb)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "pred_test = svm.predict(X_test_tfidf_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= svm, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(X_train_tfidf_emb, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(class_weight=gridsearch.best_params_['class_weight'])\n",
    "svm.fit(X_train_tfidf_emb, y_train)\n",
    "\n",
    "pred_train = svm.predict(X_train_tfidf_emb)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = svm.predict(X_test_tfidf_emb)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with W2V Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalar = StandardScaler()\n",
    "train_scaled = scalar.fit_transform(train_embeddings)\n",
    "test_scaled = scalar.fit_transform(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(train_scaled, y_train)\n",
    "\n",
    "pred_train = svm.predict(train_scaled)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "pred_test = svm.predict(test_scaled)\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handel the Imbalace Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= svm, \n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(), \n",
    "                          n_jobs=-1, \n",
    "                          scoring='f1', \n",
    "                          verbose=2).fit(train_embeddings, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "helper.plot_score_for_weight(gridsearch, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(class_weight=gridsearch.best_params_['class_weight'])\n",
    "svm.fit(train_scaled, y_train)\n",
    "\n",
    "pred_train = svm.predict(train_scaled)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "# Predicting on the test data\n",
    "pred_test = svm.predict(test_scaled)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojis Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZCw3IutWZAK"
   },
   "outputs": [],
   "source": [
    "def extract_emojis(tweet):\n",
    "    return list(emojis.get(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_train = df_train['tweet'].apply(extract_emojis)\n",
    "emo_test = df_test['tweet'].apply(extract_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji2vec_path = \"emoji2vec.bin\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(emoji2vec_path, binary=True, unicode_errors='ignore')   \n",
    "\n",
    "\n",
    "train_emo_embeddings = helper.get_word2vec_embeddings(model, emo_train)\n",
    "test_emo_embeddings = helper.get_word2vec_embeddings(model, emo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word_Vector o Emoji_Vector\n",
    "\n",
    "x_train_features_concat = []\n",
    "for t, e in zip(train_scaled, train_emo_embeddings):\n",
    "    x_train_features_concat.append(np.concatenate((t, e), axis=0))\n",
    "    \n",
    "# Word_Vector + Emoji_Vector\n",
    "x_train_features_sum = []\n",
    "for t, e in zip(train_scaled, train_emo_embeddings):\n",
    "    x_train_features_sum.append(t + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word_Vector o Emoji_Vector\n",
    "x_test_features_concat = []\n",
    "for t, e in zip(test_scaled, test_emo_embeddings):\n",
    "    x_test_features_concat.append(np.concatenate((t, e), axis=0))\n",
    "\n",
    "# Word_Vector + Emoji_Vector\n",
    "x_test_features_sum = []\n",
    "for t, e in zip(test_scaled, test_emo_embeddings):\n",
    "    x_test_features_sum.append(t + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(x_train_features_sum, y_train)\n",
    "\n",
    "pred_train = svm.predict(x_train_features_sum)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "\n",
    "pred_test = svm.predict(x_test_features_sum)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7TXXuoock3D"
   },
   "outputs": [],
   "source": [
    "svm = SVC(class_weight='balanced')\n",
    "svm.fit(x_train_features_concat, y_train)\n",
    "\n",
    "pred_train = svm.predict(x_train_features_concat)\n",
    "helper.print_train_scores(y_train, pred_train)\n",
    "print('*'*50)\n",
    "\n",
    "\n",
    "pred_test = svm.predict(x_test_features_concat)\n",
    "\n",
    "#Calculating and printing the scores \n",
    "helper.print_test_scores(y_test, pred_test)\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "helper.print_confusion_matrix(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOdzH76ebUOfNLvk/Mm+zKm",
   "collapsed_sections": [
    "T8zVKs3-aWXa",
    "zEaWJd9T-HYQ",
    "eiu03Jp5gg9v",
    "tuqo7pjfchHn"
   ],
   "name": "GP First Trail.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
